{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "FORMAT = '%(asctime)s %(levelname)s: %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, filename='myLog.log', format=FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 260601\n",
      "train data size: 86868\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_values.csv', index_col=0)\n",
    "test_data = pd.read_csv('test_values.csv', index_col=0)\n",
    "print(f'train data size: {train_data.shape[0]}')\n",
    "print(f'train data size: {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate report\n",
    "# import pandas_profiling\n",
    "# total_data = pd.concat([train_data, test_data])\n",
    "# total_data.head()\n",
    "# profile = pandas_profiling.ProfileReport(total_data)\n",
    "# profile.to_file('total_data_profile.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Heal with geo_level_1_id, geo_level_2_id, geo_level_3_id\n",
    "\n",
    "Use autoencoder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoOutputNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out_1, D_out_2):\n",
    "        super(TwoOutputNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out_1)\n",
    "        self.fc3 = nn.Linear(H, D_out_2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        output_1 = torch.sigmoid(self.fc2(x))\n",
    "        output_2 = torch.sigmoid(self.fc3(x))\n",
    "        return output_1, output_2, x\n",
    "\n",
    "def train_geo_autoencoder():\n",
    "    tmp = pd.concat([train_data,test_data])\n",
    "    x = pd.get_dummies(tmp['geo_level_3_id']).to_numpy()\n",
    "    y_1 = pd.get_dummies(tmp['geo_level_2_id']).to_numpy()\n",
    "    y_2 = pd.get_dummies(tmp['geo_level_1_id']).to_numpy()\n",
    "\n",
    "    x = torch.from_numpy(x)\n",
    "    y_1 = torch.from_numpy(y_1)\n",
    "    y_2 = torch.from_numpy(y_2)\n",
    "    x = x.type(torch.FloatTensor)\n",
    "    y_1 = y_1.type(torch.FloatTensor)\n",
    "    y_2 = y_2.type(torch.FloatTensor)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(x, y_1, y_2)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                             batch_size=128,\n",
    "                                             shuffle=True, \n",
    "                                             num_workers=2)\n",
    "\n",
    "    model = TwoOutputNet(x.shape[1], 16, y_1.shape[1], y_2.shape[1])\n",
    "\n",
    "    loss_f = torch.nn.BCELoss()\n",
    "    loss_f_1 = torch.nn.BCELoss()\n",
    "\n",
    "    lr = 1e-2\n",
    "    optimzer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for e in range(10):\n",
    "        for i, (x_data, y_1_data, y_2_data) in enumerate(dataloader):\n",
    "\n",
    "            out1, out2, _ = model(x_data)\n",
    "\n",
    "            loss = loss_f(out1, y_1_data) + loss_f_1(out2, y_2_data)\n",
    "            if not i%1000:\n",
    "                print(f'epoch: {e}, i: {i}, loss: {loss.item()}')\n",
    "\n",
    "            optimzer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimzer.step()\n",
    "            \n",
    "    torch.save(model.state_dict(), 'model.m')\n",
    "    return model\n",
    "\n",
    "def output_ae(model, x):\n",
    "    x = torch.tensor(x)\n",
    "    x = x.type(torch.FloatTensor)\n",
    "    return model(x)[-1]\n",
    "\n",
    "def load_model(model_path):\n",
    "    w = torch.load(model_path)\n",
    "    m = TwoOutputNet(w['fc1.weight'].shape[1], \n",
    "                     w['fc1.bias'].shape[0], \n",
    "                     w['fc2.bias'].shape[0], \n",
    "                     w['fc3.bias'].shape[0]\n",
    "                    )\n",
    "    m.load_state_dict(torch.load(model_path), strict=False)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_train_autoencoder = False\n",
    "\n",
    "if need_train_autoencoder:\n",
    "    model = train_geo_autoencoder()\n",
    "else:\n",
    "    model = load_model('model.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.concat([train_data,test_data])\n",
    "train_size = train_data.shape[0]\n",
    "geo3 = pd.get_dummies(total_data['geo_level_3_id']).to_numpy()\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for i, v in enumerate(geo3):\n",
    "    ans = output_ae(model, v)\n",
    "    ans = ans.detach().numpy() \n",
    "    tmp.append(ans)\n",
    "\n",
    "tmp = np.array(tmp).T\n",
    "\n",
    "for i in range(16):\n",
    "    train_data[f'geo_fea_{i}'] = tmp[i][:train_size]\n",
    "    test_data[f'geo_fea_{i}'] = tmp[i][train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
    "test_data = test_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv('train_labels.csv', index_col=0)\n",
    "y_train_data = pd.merge(train_data, y, on='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heal_with_feature(table):\n",
    "    return pd.get_dummies(table)\n",
    "\n",
    "def generate_submiss(model, name):\n",
    "    heal_with_test_data = heal_with_feature(test_data)\n",
    "    pred = model.predict(heal_with_test_data)\n",
    "    heal_with_test_data['damage_grade'] = pred\n",
    "    heal_with_test_data = heal_with_test_data[['damage_grade']]\n",
    "    heal_with_test_data.to_csv(f'{name}_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_data = heal_with_feature(y_train_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dum_data.drop(columns=['damage_grade']), \n",
    "                                                    dum_data['damage_grade'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=dum_data['damage_grade'],\n",
    "                                                    random_state=1114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomsearchCV\n",
    "param_grid = {\n",
    "        'n_estimators': range(500, 1501, 100),\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [2, 4, 6, 8, 10]\n",
    "}\n",
    "param = {\n",
    "#     'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.1,\n",
    "#     'max_depth': 10,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor'\n",
    "}\n",
    "m = XGBClassifier(**param)\n",
    "gs = RandomizedSearchCV(m, param_grid, n_jobs=-1, cv=6, scoring='f1_micro', n_iter=100)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.9, 'reg_lambda': 50.0, 'n_estimators': 1000, 'min_child_weight': 5.0, 'max_depth': 10, 'gamma': 0.5, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.7}\n",
      "0.7523263176071066\n",
      "0.7523263176071067\n",
      "[[ 2833  2131    61]\n",
      " [ 1111 25107  3434]\n",
      " [  119  6053 11272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.56      0.62      5025\n",
      "           2       0.75      0.85      0.80     29652\n",
      "           3       0.76      0.65      0.70     17444\n",
      "\n",
      "    accuracy                           0.75     52121\n",
      "   macro avg       0.74      0.69      0.71     52121\n",
      "weighted avg       0.75      0.75      0.75     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = gs.predict(X_test)\n",
    "logging.info(str(gs.best_params_))\n",
    "print(gs.best_params_)\n",
    "print(metrics.accuracy_score(y_test, predictions))\n",
    "print(metrics.f1_score(y_test, predictions, average='micro'))\n",
    "print(metrics.confusion_matrix(y_test, predictions))\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submiss(gs, 'xgb_autoencoder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}